---
title: 朴素贝叶斯分类
tags: 统计学习方法
---

> [贝叶斯公式参见](https://refantasy.cn/2020/03/04/随机事件与概率.html)
> 本文内容和公式主要参考 李航《统计学习方法》

## 基本思想

设$X$为随机向量，即数据的特征向量，$Y$为输出空间的随机变量，即分类结果。$P(X,Y)$是$X$和$Y$的联合概率分布，并通过此概率分布生成训练数据集。

$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

其中，先验概率分布为

$$
\begin{equation}
P(Y=c_k),\quad k=1,2,...,K
\end{equation}
$$

条件概率分布为

$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K
$$

由贝叶斯公式可知

$$
\begin{equation}
\begin{aligned}
P(Y=c_k|X = x)&=\frac{P((X=x)·(Y=c_k))}{P(X=x)}\\
              &=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
\end{aligned}
\end{equation}
$$

其中，$K$为样本类别数目。

在式$(4)$中，$x$表示任意样本实例，$c_k$表示随机变量$Y$取到的值。$P(Y=c_k|X = x)$表示输入样本$x$，输出分类标签$Y=c_k$的概率。在进行分类时，取$y=f(x)=\mathop{\arg\max}_{c_k} P(Y=c_k|X=x)$即为我们所要预测的分类结果。

注意到，在式$(4)$中，对于$\forall c_k$，分母的值都是一样的。所以

$$
y=\mathop{\arg\max}_{c_k}P(X=x|Y=c_k)P(Y=c_k)
$$

$y$ 即为所要预测的$x$的分类结果。

## 参数估计

在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X = x|Y=c_k)$。先验概率$P(Y=c_k)$的极大似然估计为

$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,...,K
$$

其中，$N$为样本个数，$I$为指示函数。

条件概率分布$P(X=x|Y=c_k)$有指数级参数，其估计实际是不可行的。事实上，假设$x^{(j)}$为随机变量$X$第$j$维的值，设其可取值有$S_j$个，$j=1,2,...,n$，$Y$的可取值有$K$个，那么条件概率分布的参数个数为$K\prod_{j=1}^nS_j$。朴素贝叶斯法对条件概率分布做了条件独立的假设，这也是“朴素”的得名由来。具体地，条件独立性假设是

$$
\begin{equation}
\begin{aligned}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\
            &=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
\end{equation}
$$

其中，

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$

$l=1,2,...,S_j$，$a_{jl}$是第$j$个特征可能取得第$l$个值。

由式$(5)(6)(7)(8)$即可求出随机变量$X=x$时的分类结果。

## 学习与分类算法

------

**算法 朴素贝叶斯算法**

------

输入：训练数据$T$，待预测实例$x$

输出：实例$x$的分类

1. 计算先验概率以及条件概率$P(Y=c_k)$，$P(X^{(j)}=a_{jl}|Y=c_k)$

2. 对于给定实例$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算

   $P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,...,K$

3. 确定实例$x$的分类

   $y=\mathop{\arg\max}_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$

------

## 贝叶斯估计

极大似然法估计可能会出现所要估计的概率为0的情况。这会影响到后验概率的计算结果，使分类产生偏差。解决这一问题是使用贝叶斯估计。具体地，条件概率的贝叶斯估计是

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda},\quad\lambda\geq 0
$$

同样地，先验概率的贝叶斯估计是

$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}
$$

## 代码实现

首先定义一个通用数据集类

```c++
/*
 *
 * 
 *    通用数据集类
 * 
 *    模板参数 D 为样本数据部分的数据类型
 *    模板参数 L 为样本标签部分的数据类型
 *    一个样布数据和一个样本标签组成一条记录
 *    默认使用 vector 容器保存数据记录
 * 
 *    License: MIT
 * 
 *    ©TDL 2020.03.13
 *    Contact:refantasy.cn
 * 
 */
#include <utility>
#include <vector>

template <typename D, typename L>
class DataSet
{
public:
    using value_type = D;
    using label_type = L;
    using record_type = std::pair<D, L>;

    DataSet() = default;

    /* *\brief 插入一个样本
     *  \param d 样本特征向量
     *  \param la 样本标签值
     */
    void Insert(const value_type &d, const label_type &la)
    {
        data.push_back({d, la});
    }

    /* *\brief 查找一个样本
     *  \param n 样本索引
     *  \return 一条记录引用
     */
    const record_type& GetRecord(size_t n) const
    {
        return data[n];
    }

    // 数据集大小
    size_t size() const { return data.size(); }

private:
    // 样本集合
    std::vector<record_type> data;
};

```

贝叶斯分类器

```c++
/*
 *
 *    朴素贝叶斯分类器 李航《统计学习方法》代码实现
 *
 *    该类仅用于研究贝叶斯分类算法，样本特征的不同特征默认采用同种数据类型（整型）表示
 *    样本类别标签必须是从零开始的连续整数
 *
 *    License: MIT
 * 
 *    ©TDL 2020.03.13
 *    Contact:refantasy.cn
 * 
 */
#include <vector>
#include <array>
#include "dataset.hpp"

/*
 * *\brief 贝叶斯分类器
 *  \param dims 样本特征的维度
 *  \param ClassesN 分类的类别:0,1,2,...,ClassesN-1
 *  \param MaxFeaValue 特征值得最大取值
 */
template <int dims, int ClassesN, int MaxFeaValue, typename FEATURE_TYPE = int>
class Bayes
{
    friend void Example_4_2();

public:
    Bayes() = default;

    /* *\param d 数据集，由若干条记录组成，每条记录由特征向量和标签组成
     *  \param lambd 贝叶斯估计参数，默认为0，通常取1
     *  \param Sj Sj[i]表示第i维特征可能取到的值得个数
     */
    Bayes(const DataSet<std::array<FEATURE_TYPE, dims>, int> &d,
          double lambda = 0,
          std::vector<int> Sj = std::vector<int>(dims, MaxFeaValue)) : _dataset(d), _lambda(lambda), _Sj(Sj)
    {
        int MaxK = d.GetRecord(0).second;
        for (int i = 0; i < d.size(); i++)
        {
            if (d.GetRecord(i).second > MaxK)
                MaxK = d.GetRecord(i).second;
        }
        K = MaxK;
    }

    // 训练
    void Train()
    {
        ComputePrior();
        ComputeConditionPrior();
    }

    // 预测
    int Pred(const std::array<FEATURE_TYPE, dims> &x)
    {
        double result[ClassesN] = {0};
        for (int k = 0; k < ClassesN; k++)
        {
            double prob = _prior_probability[k];
            for (int j = 0; j < dims; j++)
            {
                prob *= _condition_probability[k][j][x[j]];
            }

            result[k] = prob;
            //std::cout<<prob<<std::endl;
        }

        int max_index = 0;
        for (int k = 0; k < ClassesN; k++)
        {
            if (result[k] > result[max_index])
                max_index = k;
        }

        return max_index;
    }

private:
    // 计算先验概率
    void ComputePrior()
    {
        for (int k = 0; k < ClassesN; k++)
        {
            int I = 0;
            for (int i = 0; i < _dataset.size(); i++)
            {
                if (_dataset.GetRecord(i).second == k)
                    I++;
            }
            _prior_probability[k] = ((double)I + _lambda) / (_dataset.size() + (K + 1) * _lambda);
        }
    }

    // 计算条件概率
    void ComputeConditionPrior()
    {
        for (int k = 0; k < ClassesN; k++)
        {
            for (int j = 0; j < dims; j++)
            {
                for (int L = 0; L <= MaxFeaValue; L++)
                {
                    int p_x_y = 0;
                    int y_c_k = 0;
                    for (int i = 0; i < _dataset.size(); i++)
                    {
                        if (_dataset.GetRecord(i).second == k)
                        {
                            y_c_k++;
                            if (_dataset.GetRecord(i).first[j] == L)
                                p_x_y++;
                        }
                    }
                    _condition_probability[k][j][L] = ((double)p_x_y + _lambda) / (y_c_k + _Sj[j] * _lambda);
                }
            }
        }
    }

private:
    // 数据集
    DataSet<std::array<FEATURE_TYPE, dims>, int> _dataset;

    // 先验概率
    std::array<double, ClassesN> _prior_probability = {0};

    // 条件概率
    std::array<std::array<std::array<double, MaxFeaValue + 1>, dims>, ClassesN> _condition_probability = {0};

    double _lambda = 0;

    // 特征值可取值得个数
    std::vector<int> _Sj = std::vector<int>(dims, MaxFeaValue);

    // 样本标签可取到的值的个数
    int K = 0;
};
```

《统计学习方法》例4-2

```c++
void Example_4_2()
{
    constexpr int S = 1;
    constexpr int M = 2;
    constexpr int L = 3;

    // 创建数据集
    constexpr int dims = 2;
    constexpr int MaxFeaValue = 3;
    constexpr int classes = 2;

    // 标签重新映射到连续整形 y = -1 -> 0,  y = 1 -> 1
    DataSet<std::array<int, dims>, int> d;

    d.Insert({1, S}, 0);
    d.Insert({1, M}, 0);
    d.Insert({1, M}, 1);
    d.Insert({1, S}, 1);
    d.Insert({1, S}, 0);

    d.Insert({2, S}, 0);
    d.Insert({2, M}, 0);
    d.Insert({2, M}, 1);
    d.Insert({2, L}, 1);
    d.Insert({2, L}, 1);

    d.Insert({3, L}, 1);
    d.Insert({3, M}, 1);
    d.Insert({3, M}, 1);
    d.Insert({3, L}, 1);
    d.Insert({3, L}, 0);

    // 创建分类器并传入数据集
    Bayes<dims, classes, MaxFeaValue> bayes(d, 1);

    // 训练
    bayes.Train();

    // 输出先验概率
    std::cout << "先验概率" << std::endl;
    auto &prior = bayes._prior_probability;
    for (int y = 0; y < prior.size(); y++)
    {
        std::cout << "P(Y=" << y << ")=" << prior[y] << std::endl;
    }
    std::cout << std::endl;

    // 输出条件概率
    std::cout << "条件概率(SML分别用123替换)" << std::endl;
    auto &condi = bayes._condition_probability;
    auto f = [](int i) {if(i==1)return "S";else if(i==2)return "M";else return "L"; };
    auto g = [](int y) {if(y==0)return -1;else return 1; };
    for (int y = 0; y < prior.size(); y++)
    {
        for (int j = 0; j < dims; j++)
        {
            for (int L = 1; L <= MaxFeaValue; L++)
            {
                if (j == 1)
                {
                    std::cout << "P(X"
                              << "^" << j + 1 << "=" << f(L) << "|Y=" << g(y) << ") = " << condi[y][j][L] << std::endl;
                }
                else
                {
                    std::cout << "P(X"
                              << "^" << j + 1 << "=" << L << "|Y=" << g(y) << ") = " << condi[y][j][L] << std::endl;
                }
            }
        }
    }

    // 预测
    std::cout << "预测" << std::endl;
    std::array<int, 2> x{2, S};
    int Y = bayes.Pred(x);
    std::cout << "P(x=(2,S)) = ";
    std::cout << g(Y) << std::endl;
}
```

